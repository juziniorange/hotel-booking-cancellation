---
title: "ASML3 Lab 2 -- Basic modelling in R"
output: html_notebook
---

## Introduction

In this lab we will use two data sets to explore basic binary and multiclass
classification models that we saw in lectures so far.

The first is a credit scoring dataset: an important application in banking. 
The data consist of 4454 observations of 14 variables, each a loan that was
given together with the status of whether the loan was "good" (ie repaid) or
went "bad" (ie defaulted).

```{r}
# Uncomment and run the following command first if you do not have the modeldata package
# install.packages("modeldata")
data("credit_data", package = "modeldata")
```

Use some of the exploration techniques from Lab 1 to explore the dataset
before you start modelling.

```{r}
# Try some plotting function to explore the data here

```

The second dataset we will use which is multiclass is the famous MNIST
handwriting data set.  I have already put this data into the format we
discussed in lectures for coding of images (an image per row with pixels as
features).  Download it first (this may take some time, it is ~40MB)

```{r}
# Uncomment and run the following command first if you do not have the fst package
# install.packages("fst")
download.file("https://www.louisaslett.com/Courses/MISCADA/mnist.fst", "mnist.fst")
mnist <- fst::read.fst("mnist.fst")
```

Have a look at this data.  If you want to see an image, the following function
will help plot any of the images for you.  Read and try to understand the two
lines involved by looking at the documentation for each step.

```{r}
library("tidyverse")

plotimages <- function(i) {
  imgs <- mnist %>% 
    slice(i) %>% 
    mutate(image.num = i) %>%
    pivot_longer(x0.y27:x27.y0,
                 names_to = c("x", "y"),
                 names_pattern = "x([0-9]{1,2}).y([0-9]{1,2})",
                 names_transform = list(x = as.integer, y = as.integer),
                 values_to = "greyscale")
  
  ggplot(imgs, aes(x = x, y = y, fill = greyscale)) +
    geom_tile() +
    scale_fill_gradient(low = "white", high = "black") +
    facet_wrap(~ image.num + response, labeller = label_both)
}
```

We can then look at the individual images, or groups of them.  For example, to 
look at images 21 through 24, we run:

```{r}
plotimages(21:24)
```



## Base R linear methods for credit data

We saw a quick fit of logistic regression to the hotels data at the end of the
last lab but it was a bit rushed!  We'll revisit logistic regression again here
going more slowly and understanding the details for the banking data and then
try LDA.

### Logistic regression

We can fit a standard logistic regression to the credit data using the `glm()`
function in base R.  Note that logistic regression is a special case of a
family of models called Generalised Linear Models which arises from assuming a
Binomially distributed response (we do not go into this mathematical detail in
lectures), so we specify the `binomial` family argument.

```{r}
credit_fit <- glm(Status ~ ., binomial, credit_data)
```

You can inspect the fitted coefficients by looking at the object returned or
get more detail (such as statistical tests of the coefficients, see ASML2) by
passing the object to `summary()`.

```{r}
credit_fit
summary(credit_fit)
```

If we want to predict using this data we use the predict function.  

```{r}
credit_pred <- predict(credit_fit, credit_data)
credit_pred
```

These values might look a bit odd if you were expecting probabilities!  By
default, predict returns the *linear* part.  If we want the probabilities we
need to ask for the response:

```{r}
credit_pred <- predict(credit_fit, credit_data, type = "response")
credit_pred
```

If we want to mimic the Bayes classifier, we then need to choose:

$$ \hat{y} = \arg\max_k \mathbb{P}(Y = C_k | X = x) $$

But what are the $C_k$?  It turns out that `glm()` assigns 0/1 to the factor
levels in the order they are in the data.

```{r}
# Check the order ...
levels(credit_data$Status)
# ... so "bad" = 0 and "good" = 1

# Now use this knowledge to set Bayes classifier prediction
y_hat <- factor(ifelse(credit_pred > 0.5, "good", "bad"))
```

Once we have this we can compute the accuracy of the classifier on the data:

```{r}
mean(I(y_hat == credit_data$Status))
```

What went wrong??

It turns out that there is missing data within the credit data and one of the
limiting aspects of logistic regression is that it needs complete data to be
able to make a prediction (otherwise what to multiply by the coefficient?).  We
will omit the missing predictions,

```{r}
mean(I(na.omit(y_hat == credit_data$Status)))
```

This compound estimate of accuracy is not very revealing.  A much more
insightful display is a confusion matrix:

```{r}
table(truth = na.omit(credit_data)$Status, prediction = na.omit(y_hat))
```



### Linear Discriminant Analysis

We can also try the first generative modelling approach we have seen in
lectures, LDA.  Printing the object after fitting reveals the class
probabilities, group means, etc.

```{r}
credit_lda <- MASS::lda(Status ~ ., credit_data)
credit_lda
```

We can again evaluate the fit.  LDA is also unable to handle missing values, so
we omit them.

```{r}
credit_pred <- predict(credit_lda, na.omit(credit_data))
credit_pred
```

Notice this prediction object is totally different to that provided by logistic
regression.  It is a list containing two variables, `class` and `posterior`, so
here we don't need to compute the Bayes classifier as it is done for us.

The accuracy now is:

```{r}
mean(I(credit_pred$class == na.omit(credit_data)$Status))
```

The confusion matrix in this case is:

```{r}
table(truth = na.omit(credit_data)$Status, prediction = credit_pred$class)
```

This feels rather unpleasant ... it seems each model is going to have its
own interface and its own method of outputting predictions.  Whilst it is good
to understand how to access these methods directly, since we are going to learn
several more techniques over the coming lectures it would be annoying to have
to learn separate interfaces for them all!

Fortunately, there are meta-packages which make life easier.  The one we will
look at here is called `mlr3` (https://mlr3.mlr-org.com) and is arguably the
gold standard in machine learning meta-packages.  

If you want to investigate more yourself, there are others: `caret`
(http://topepo.github.io/caret/index.html) is a slightly older one.  The
authors of `caret` have now moved on to develop `tidymodels`
(https://github.com/tidymodels) which is designed to supersede it, but it does
not yet have all the functionality of `caret`.  The old version of `mlr`
(https://mlr.mlr-org.com) is still very good and has more functionality than
has been brought into `mlr3` so far, but it will be out of date soon so we
favour learning `mlr3` here.



## MLR 3

All of the above analysis can be done using `mlr3` as follows.

MLR breaks machine learning problems up into steps.  The most basic ones are
where you:

- define tasks
- define learners
- train learners on tasks
- predict from learners
- evaluate performance

### Credit data

To define the credit task, we provide the data and specify what is the target
(response variable for learning).

```{r}
library("mlr3")

task_credit <- TaskClassif$new(id = "credit",
                               backend = na.omit(credit_data),
                               target = "Status")
task_credit
```

Then, we can see what learners MLR has built in.

```{r}
as.data.table(mlr_learners)
```

This seems rather few!  This is because additional packages are used to add
features such as new learners.  The staple learners are in the `mlr3learners`
add on package, which then updates `mlr_learners` with a lot of additional
options.

```{r}
library("mlr3learners")
#library("mlr3proba") # No longer on CRAN, but useful for survival and regression models (see website)

as.data.table(mlr_learners)
```

We will define a logistic regression learner.

```{r}
learner_lr <- lrn("classif.log_reg")
learner_lr
```

Then we train that learner on the credit data task.

```{r}
learner_lr$train(task_credit)
```

And then predict on the same data (see lecture why this is actually a bad
idea!)

```{r}
pred <- learner_lr$predict(task_credit)
pred
```

Finally, we can assess accuracy and confusion matrix by accessing this object.

```{r}
pred$score(msr("classif.acc"))
pred$confusion
```

There are many more ways to evaluate accuracy, some of which we'll encounter in
lectures.  See the many MLR3 supports using this:

```{r}
mlr_measures
```

We can do the same for the LDA, this time with all the code at once for
readability (we do not need to recreate the task).

```{r}
learner_lda <- lrn("classif.lda")

learner_lda$train(task_credit)

pred_lda <- learner_lda$predict(task_credit)

pred_lda$score(msr("classif.acc"))
pred_lda$confusion
```

Thus, a unified interface makes life a *lot* easier!



### MNIST

With this new-found ease of fitting, we can try a final quick experiment on the
more exciting MNIST data.  Note that this is a very large dataset, so for the
purposes of the lab we are only going to use a random sample of 1000 images. 
You can try the bigger dataset on your own computer in your own time.  We also
need to set the response to be a factor because R loaded it as numbers.

```{r}
mnist <- mnist %>%
  slice(sample(length(response), 1000)) %>% 
  mutate(response = as.factor(response))

task_mnist <- TaskClassif$new(id = "credit",
                              backend = mnist,
                              target = "response")

learner_glmnet <- lrn("classif.glmnet")

learner_glmnet$train(task_mnist)

pred_glmnet <- learner_glmnet$predict(task_mnist)

pred_glmnet$score(msr("classif.acc"))
pred_glmnet$confusion
```

Let's look at an example of what an image it got wrong looks like.  Try to
understand how this code produces what you want.

```{r}
wrong <- pred_glmnet$truth!=pred_glmnet$response

# Change i to look at different wrong predictions
i <- 2
plotimages(which(wrong)[i]) +
  ggtitle(glue::glue("Predicted {pred_glmnet$response[which(wrong)[i]]}, but actually {pred_glmnet$truth[which(wrong)[i]]}"))
```

Can you see how the algorithm might have been confused?

Next we can compare LDA ...

```{r}
learner_lda <- lrn("classif.lda")

task_mnist <- TaskClassif$new(id = "credit",
                              backend = mnist,
                              target = "response")

#learner_lda$train(task_mnist) # NB this gives an error ... that is intended, see below!
```

We have a problem: some of the pixels are always the same colour (eg in the
corners of the image there is never anything written) and this causes the
covariance to be uncomputable there.  We must first remove these features and
then fit again.

```{r}
task_mnist <- TaskClassif$new(id = "credit",
                              backend = mnist %>%
                                select_if(function(x) {
                                  is.factor(x) || I(sd(x)>0)
                                }),
                              target = "response")

learner_lda$train(task_mnist)

pred_lda <- learner_lda$predict(task_mnist)

pred_lda$score(msr("classif.acc"))
pred_lda$confusion
```

And have a look at one LDA got wrong:

```{r}
wrong <- pred_lda$truth!=pred_lda$response

# Change i to look at different wrong predictions
i <- 2
plotimages(which(wrong)[i]) +
  ggtitle(glue::glue("Predicted {pred_lda$response[which(wrong)[i]]}, but actually {pred_lda$truth[which(wrong)[i]]}"))
```

# Wrap-up

What is the big thing wrong/missing in the above???

Yes, error estimation!  We have not properly performed cross validation or some
other method of correctly estimating the error ... so we'll be digging into that
and construct a full pipeline for analysis in the next lab!
