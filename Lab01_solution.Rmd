---
title: "ASML3 Lab 1 -- Taster of modern R for data manipulation and visualisation"
output:
  html_document:
    df_print: paged
---

This is the first lab so we want to take the opportunity to look at some data
exploration: although the maths and modelling is exciting, data exploration is
absolutely critical to any real-world application.  Never blindly run into
building a machine learning model without exploring your data first!!

We are also going to introduce the modern approach to using R which sets it
apart from other data science languages in functionality and ease.  This is a
quick taster, because there are a rich collection of tools which would take 
many hours to teach in a lab.  The syntax may be very unfamiliar at first, but 
it makes modern R code substantially more readable and understandable than base
R or other languages.

It is highly recommended that you read the free online book at 
<https://r4ds.had.co.nz/> in your own time if you want to get better at data 
manipulation and visualisation in R, since we have limited time and will focus 
on the modelling aspects in the remaining labs.

First, we download a dataset on hotel bookings.  The `readr` package reads in
data to the updated version of a data frame, called a tibble and performs more
checks that the standard R `read.csv()`.  We then use `View()` to have a look
at the data in the RStudio viewer.

```{r}
hotels <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv")
View(hotels)
```

Notice the function above might look unusual `readr::read_csv(...)`.  This is
the syntax to run a function (`read_csv`) from inside a package (`readr`) 
without having to load the whole package into our session.  When you only plan 
to use one or two functions in a package this is a good idea to avoid possible 
function name clashes.

Open the web page here to read about this data set:

https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md

This data is from a [#tidytuesday](https://twitter.com/search?q=%23tidytuesday)
challenge.  These are fun challenges which run most weeks on Twitter where
people post their exploration of a different data set which is proposed each
week.  I highly encourage you to follow and participate in them if you want to
improve your general data science skills (wrangling and visualisation
especially) ... you can learn a lot about these hands on aspects of data science
from looking at what people do.  This course will be more about machine learning
modelling from here on out.

First, let's have a look at the summary of the data using the `skimr` package.

```{r}
library("skimr")
skim(hotels)
```

`skimr` is a great way to get an immediate feel for the data and integrates well
with other tools we will use later.  For example, if you later knit to a web
document, the above summary is formatted into a proper HTML table.

We will make use of the `tidyverse` of packages, in particular `dplyr` for data
manipulation.  Loading the `tidyverse` automatically loads a collection of
related packages.  We will also use `ggplot` for visualisation.

```{r}
library("tidyverse")
library("ggplot2")
```

Let's immediately try out a simple scatterplot.  The variable `adr` is the
Average Daily Rate (=total price of stay/number of nights).  Let's see if this
relates at all to the length of the stay, which would be held in
`stays_in_weekend_nights` and `stays_in_week_nights`.

The first argument to `ggplot()` is the dataset where all later variables
referenced can be found.  The second argument is wrapped in an `aes()`
function call and specifies the aesthetics (where each part of the plot should
be found).  After this, we add (`+`) a layer, in this case requesting a scatter
plot with `geom_point()`.

Note that R ignores whitespace, so it is highly recommended to liberally use
returns, indenting and spaces to make your code easier to read, like this:

```{r}
ggplot(hotels,
       aes(x = adr, y = stays_in_weekend_nights+stays_in_week_nights)) +
  geom_point()
```

Clearly there was one very expensive room booking!

We can easily use `dplyr` to manipulate the data.  For example:

- `filter()` enables us to subset data, say by specifying only `adr`'s below
    4000.
- `mutate()` enables us to change variables or add new ones, so we could compute
    a new variable `total_nights` rather than do the sum in the plot command.

`dplyr` uses the pipe operator `|>` in order to string together different
commands and we can do this right in line with the plot command.

We'll change one other thing ... there is a lot of overplotting, so we can make
the scatter plot partially transparent with the `alpha = 0.1` argument.

```{r}
ggplot(hotels |>
         filter(adr < 4000) |> 
         mutate(total_nights = stays_in_weekend_nights+stays_in_week_nights),
       aes(x = adr, y = total_nights)) +
  geom_point(alpha=0.1)
```

Notice that the `hotels` data frame has not actually changed, the result of the
`dplyr` commands are passed straight to the `ggplot()` function.  We can store
these results by instead writing them back to the original data frame, or to 
somewhere new:

```{r}
hotels <- hotels |>
  filter(adr < 4000) |> 
  mutate(total_nights = stays_in_weekend_nights+stays_in_week_nights)
```

In the plot it is clearly still hard to see, so we can just switch from a scatter plot
(`geom_point()`) to a heatmap (`geom_bin2d()`), where we set the bins to be in
cost size 10 and 1 night.  Now, we just need to provide the data frame as we
have saved the results of our manipulation.

We can layer on top other elements, for example we can add a simple smoothed
line for the expected number of nights at each price point to see if there is
actually any likely trend by just adding a `geom_smooth()` to the plot.

```{r}
ggplot(hotels,
       aes(x = adr, y = total_nights)) +
  geom_bin2d(binwidth=c(10,1)) +
  geom_smooth()
```

Let us focus now on some more data manipulation in preparation for trying to
see if we can predict in advance what bookings might get cancelled.  Looking
at the data, we need to tidy things up a little, including:

- eliminating variables that are not appropriate to use for prediction ... for
    example, if you read the web page earlier describing the variables, we
    should clearly eliminiate `reservation_status` and `reservation_status_date`
- we may choose to transform other variables, for example rather than having a
    total number of children and of babies, perhaps we just want an indicator
    for there being kids in the booking.

We will use `dplyr` to manipulate the data in this way.  We saw `filter()` and
`mutate()` above.  We will learn 2 more of these:

- `select` can be used to choose among or to remove variables.  Putting a minus
    in front indicates removal.
- `case_when` is used to return one of a set of possible values based on a
    condition.  Each observation is compared against the condition on the left
    of the tilde `~` symbol *in order* and the first matching case results in
    the expression on the right being returned.

For example, let's perform the above steps:

```{r}
hotels <- hotels |>
  select(-reservation_status, -reservation_status_date) |> 
  mutate(kids = case_when(
    children + babies > 0 ~ "kids",
    TRUE ~ "none"
  ))
```

Now that we have our indicator for kids, we don't actually need the `babies` and
`children` variables any more.  Replace each `?` below with the correct code to
remove them.

```{r,eval=FALSE}
hotels <- hotels |> 
  ?(?)
```
```{r}
hotels <- hotels |> 
  select(-babies, -children)
```

We are also not interested in how many parking spaces are needed, just if
one was requested or not.  Replace each `?` below with the correct code to:

- create a new variable called `parking` which is either "parking" or "none"
    depending on the `required_car_parking_spaces` variable value
- then remove the `required_car_parking_spaces` variable.

```{r,eval=FALSE}
hotels <- hotels |> 
  mutate(? = case_when(
    ? ~ ?,
    TRUE ~ "none"
  )) |> 
  select(?)
```
```{r}
hotels <- hotels |> 
  mutate(parking = case_when(
    required_car_parking_spaces > 0 ~ "parking",
    TRUE ~ "none"
  )) |> 
  select(-required_car_parking_spaces)
```

Look at the data and confirm that all the above steps have happened.

It can be time consuming to check lots of plots like above one-by-one.  Instead,
we can look at a pairs plot to examine the pairwise relationships between some 
variables of interest.  For example,

```{r}
library("GGally")

ggpairs(hotels |> select(kids, adr, parking, total_of_special_requests),
        aes(color = kids))
```

Can you interpret what is being plotted here?  Ask for help if not!

We could check to see if different countries have different cancellation rates
by manipulating the data.  To do this, we need to learn further `dplyr` 
commands:

- `group_by` specifies the variables that you want to construct summary 
    information by
- `summarise` constructs new summary statistics computed for each group defined
    by the grouping variables.  There are common functions such as `n()` (total
    number in that group) and `sum()` (adding up another variable) and `mean()`
    etc.  Notice that you can also reference variables you just created.

```{r}
hotels.bycountry <- hotels |> 
  group_by(country) |> 
  summarise(total = n(),
            cancellations = sum(is_canceled),
            pct.cancelled = cancellations/total*100)
```

Inspect the new data frame `hotels.bycountry`.  There are a lot of countries,
but we could visualise the top 10 by the total number of bookings.

- `arrange()` allows us to sort (ascending by default, descending if you use
    `desc()`)
- `head()` retains just the specified number of rows.

```{r}
ggplot(hotels.bycountry |> arrange(desc(total)) |> head(10),
       aes(x = country, y = pct.cancelled)) +
  geom_col()
```

However, it may be most interesting to view this on a world map, which can also
be easily achieved with `ggplot`.

First, we load mapping data from associated libraries

```{r}
library("rnaturalearth")
library("rnaturalearthdata")
library("rgeos")

world <- ne_countries(scale = "medium", returnclass = "sf")
```

If you inspect the `world` object you will find mapping information about 241 
countries.  In particular, the `iso_a3` variable appears to contain ISO country
code like the hotel data.

All we need to do is to attach the `hotels.bycountry` data to the world data
using the ISO codes in the `country` variable to match rows.

This involves the `dplyr` function `left_join()`.  This function takes the name
of the data frame you want to attach and then the names of the key variable to
match in each data frame.  Crucially, whichever data frame is fed to `left_join()` will retain all its rows, even when there is no matching row in the data frame being joined to it (so we won't lose parts of the map from which nobody booked a room)

```{r}
world2 <- world |>
  left_join(hotels.bycountry,
            by = c("iso_a3" = "country"))
```

Inspect the data frame and see that the `total`, `cancellations` and `pct.cancelled` variables have all been added.  Notice that any country that
was not present now has `NAs` for these values.

We can now plot the map with `ggplot` and set the fill colour of each country
to be the percentage of customers who cancelled.

```{r}
ggplot(world2) +
  geom_sf(aes(fill = pct.cancelled))
```

Finally, we can look at the relation between different discrete variables with
a parallel sets diagram, by cancellation status.

We do this by first summarising the discrete levels we want to examine like
before:

```{r}
hotels.par <- hotels |>
  select(hotel, is_canceled, kids, meal, customer_type) |>
  group_by(hotel, is_canceled, kids, meal, customer_type) |>
  summarize(value = n())
```

Inspect the `hotels.par` object and make sure you understand what this code has
done.  Now we can visualise the relationship between levels and the 
cancellation status:

```{r}
library("ggforce")

ggplot(hotels.par |> gather_set_data(x = c(1, 3:5)),
       aes(x = x, id = id, split = y, value = value)) +
  geom_parallel_sets(aes(fill = as.factor(is_canceled)),
                     axis.width = 0.1,
                     alpha = 0.66) + 
  geom_parallel_sets_axes(axis.width = 0.15, fill = "lightgrey") + 
  geom_parallel_sets_labels(angle = 0) +
  coord_flip()
```



## Fitting a first statistical machine learning model

We will now fit a logistic regression model as seen in lectures.  Now, note that
we have a lot of categorical variables, so as we know from the second lecture
these need to be made numeric and that for a linear model like logistic 
regression we should use a *dummy coding*.  Fortunately for us, the built in
R function `glm()` will do this for us.  However, look back at the output of
`skim` right at the start of this practical: the first group of variables (of
type `character`) are the categorical ones.  Do you think we should remove some
of these?  Why?  (Hint: look at the `n_unique` column)

ANSWER: Yes, some of the categorical variables have a lot of unique levels and
        when we create a dummy encoding we will end up with many more variables
        in the model.  For example, `agent` has 334 unique values, so we would
        turn `agent` from a single variable into 333 new variables!

Remove:

- all categorical variables with 10 or more unique levels, except for
  `arrival_date_month`.
- `stays_in_weekend_nights` and `stays_in_week_nights`.  Why?

ANSWER: the two stays variables are a linear combination of the new variable
        we created earlier in the lab (total nights) and so the design matrix
        would be singular and cause problems with fitting.

```{r}
hotels2 <- hotels |> 
  select(-country, -reserved_room_type, -assigned_room_type, -agent, -company,
         -stays_in_weekend_nights, -stays_in_week_nights)
```

We can now fit a logisitic regression model to predict the `is_canceled` 
variable based on all others in the data set.  We tell R this is the goal by
first converting `is_canceled` to a `factor` type and then make a *formula* by
typing `as.factor(is_canceled) ~ .` ... the `~` symbol is read "is predicted by"
and the `.` is read "everything else in the dataset".  So:

*`is_canceled` is a factor which is predicted by everything else in the dataset*

We could also manually list variables separated by `+` but there are a lot!

```{r}
fit.lr <- glm(as.factor(is_canceled) ~ ., binomial, hotels2)
```

The model is now stored in `fit.lr`.  Have a look at the detailed output:

```{r}
summary(fit.lr)
```

Do you think the sign and significance of the different predictors makes
intuitive sense?

ANSWER: It looks like it does in many cases.  For example, `is_repeated_guest`
        is negative, meaning repeated guests are less likely to cancel.  Also,
        `previous_cancellations` is positive, meaning people who have cancelled
        before are more likely to cancel again.  This and many other fitted
        coefficients seem to be logical.

We could carry on now to look at the predictions the model makes.  We ask for
`type="response"` so that we get probabilities and not the linear model part:

```{r}
pred.lr <- predict(fit.lr, hotels2, type = "response")
```

We might even look at a histogram plot of these predictions:

```{r}
ggplot(data.frame(x = pred.lr), aes(x = x)) + geom_histogram()
```

We can see that the model seems to make some very confident predictions of
cancellations and non-cancellations, with a skewed spread of more uncertain
predictions.  It seems very sure of around 22 thousand, so of roughly 18% of the
data and then less certain of the rest.

Let's see how it does by comparing these predictions to the actual responses.
Hoping that we have got a "true" model here, we'll just use the Bayes prediction
in other words, take the arg max over classes.  That means we predict cancellation
if the probability of cancellation is over 0.5, but of course we do not *have*
to make this choice.

```{r}
conf.mat <- table(`true cancel` = hotels2$is_canceled, `predict cancel` = pred.lr > 0.5)
conf.mat
```

The above is called a "confusion matrix" and we can read off for example:

- Of the 75166 cancellations, we predicted 70130 correctly and got 5036 wrong
- Of the 44223 cancellations, we predicted 26498 correctly and got 17725 wrong
- When we predict cancellation, we get it right 26498/(26498+5036) = 84% of the time
- When we predict no cancellation, we get it right 70130/(70130+17725) = 80% of the time

The first two you read row-wise, the second two you read column wise.  Our total
overall accuracy is (70130+26498)/(70130+26498+17725+5036) = 81%

We can then output for example the class conditional accuracy percentages as:

```{r}
conf.mat/rowSums(conf.mat)*100
```

Do you think this model could be useful to a hotel chain or booking website?

Imagine that the hotel policy is only to charge one night if there is a
cancellation and to refund the other nights.  Then, we could setup our model to
overbook capacity when people appear likely to cancel (for example is is common
to overbook in the airline industry).  How might you explore this?

Some important final comments:

- What we have done above, calculating the accuracies on the same data we used
  to fit the model, is **NOT** a good idea.  We will see in lectures why it is 
  bad and how to do better!
- All the different accuracies we tried to calculate above are super important
  in a variety of different ways.  Again, we'll learn more about sensitivity,
  specificity, etc in lectures soon.

Using these results, explore the plotting options at:

https://ggplot2.tidyverse.org/

and the data manipulation reference at:

https://dplyr.tidyverse.org/

to explore the relationship between the variables rated important and the 
bookings that are cancelled.

If you exhaust this, then explore some of the visualisations others presented on
Twitter at [#tidytuesday](https://twitter.com/search?q=%23tidytuesday).
