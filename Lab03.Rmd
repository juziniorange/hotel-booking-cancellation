---
title: "ASML3 Lab 3 -- Full modelling exercise"
output: html_notebook
---

## Introduction

We will continue looking at the credit dataset from the last lab and do a full analysis using the machine learning techniques that we have learned so far.  Previous labs have gone a little slower and focused on detail.  Today, we will dive deeper in to MLR3 and run a best-practise analysis, which may require you to spend time outside the lab to fully understand.

First, load the dataset into R:

```{r}
# Credit data
data("credit_data", package = "modeldata")
```



## Credit data

### Data exploration

First, one should always familiarise oneself with the data at hand.  The following is probably an absolute minimum.

```{r}
skimr::skim(credit_data)
```

The important thing to note from above is that some variables have missing values.  This is common in real world datasets and so we're going to work hard to deal with that without throwing data away.

Now some plots:

```{r}
DataExplorer::plot_bar(credit_data, ncol = 3)
```

```{r}
DataExplorer::plot_histogram(credit_data, ncol = 3)
```

The final one splits out continuous variables by the response we are interested in modelling:

```{r}
DataExplorer::plot_boxplot(credit_data, by = "Status", ncol = 3)
```



### Task and resampling

First, let's load MLR3 and all related tools, using the `mlr3verse` meta-package.  We will also need the `data.table` package which is the main data structure used by MLR3 (don't worry if you get some red text warning about OpenMP support).

```{r}
library("data.table")
library("mlr3verse")
```

The first job when we turn to modelling in MLR3 is always to define a task.  Here, we are going to define a 'positive' outcome to be a loan going bad.  This will be important to specify for when we assess the model performance.

```{r}
set.seed(212) # set seed for reproducibility
credit_task <- TaskClassif$new(id = "BankCredit",
                               backend = credit_data, # <- NB: no na.omit() this time
                               target = "Status",
                               positive = "bad")
```

Notice above that this time we did *not* remove the missing data as we did in lab 2.  We're going to handle it better this time.

Next, we want to define the resampling strategy we will use for error estimation.

```{r}
# Let's see what resampling strategies MLR supports

# View will open a tab where you can more easily see the detail ...
View(as.data.table(mlr_resamplings))
# ... whilst this just gives the names
mlr_resamplings

# To see help on any of them, prefix the key name with mlr_resamplings_
?mlr_resamplings_cv
```

Per lectures, we'll go for cross validation, and we'll arbitrarily choose 5 folds in this case.  We first create the resampling strategy and then we 'instantiate' it on the task.  Instantiating means that the folds get fixed: this means we can then use this object on multiple learners and know that the same set of folds have been used so that comparisons are fair.

```{r}
# The rsmp function constructs a resampling strategy, taking the name given
# above and allowing any options listed there to be chosen
cv5 <- rsmp("cv", folds = 5)
cv5$instantiate(credit_task)
```



### Some simple learners

We are now ready to try fitting some simple learners.  Let's remind ourselves what's available:

```{r}
# View will open a tab where you can more easily see the detail ...
View(as.data.table(mlr_learners))
# ... whilst this just gives the names
mlr_learners

# Again, to see help on any of them, prefix the key name with mlr_learners_
?mlr_learners_classif.log_reg
```

In the View tab, pay particular attention to the `feature_types` and `properties` columns.  This gives us information about the capabilities of different learners.  For example, we know that we have `numeric` and `factor` data from the exploration above, so only those learners listing these under `feature_types` can handle our data natively --- for other learners we have some work to do.  Likewise, we know that we have missing values, so only those learners with `missings` listed under `properties` will handle our data without any additional work.

From the list we can see two models that support both factors and missings, so we could try these first

- `classif.featureless` is a so-called baseline classifier ... it basically just predicts the most common response all the time ignoring the features!  It is often a good idea to include this because if you don't beat it then there is something very wrong!
- `classif.rpart` does classification trees using the methodology we saw in the lectures.

In both cases we're going to ask for probabilistic prediction (the default just predicts the label)

```{r}
lrn_baseline <- lrn("classif.featureless", predict_type = "prob")
lrn_cart <- lrn("classif.rpart", predict_type = "prob")
```

We can see if these have any options or hyperparameters we can change too:

```{r}
# Have a look at what options and hyperparameters the model possesses
lrn_baseline$param_set
lrn_cart$param_set
```

Let's now fit these learners using cross-validation to determine the accuracy.

```{r}
res_baseline <- resample(credit_task, lrn_baseline, cv5, store_models = TRUE)
res_cart <- resample(credit_task, lrn_cart, cv5, store_models = TRUE)

# Look at accuracy
res_baseline$aggregate()
res_cart$aggregate()
```

When we want to do multiple models it is slightly more convenient to use the benchmark function to run them all:

```{r}
res <- benchmark(data.table(
  task       = list(credit_task),
  learner    = list(lrn_baseline,
                    lrn_cart),
  resampling = list(cv5)
), store_models = TRUE)
res
res$aggregate()
```

What is `classif.ce`?  It is the mean misclassification error as defined in lectures.  Note that the error in the baseline classifier is only 0.282!  So this gives us an idea of what we need to beat (we might have expected 0.5, but this is an imbalanced dataset as is often the case with real data).

We can get many other error measures:

```{r}
# View will open a tab where you can more easily see the detail ...
View(as.data.table(mlr_measures))
# ... whilst this just gives the names
mlr_measures

# Again, to see help on any of them, prefix the key name with mlr_measures_
?mlr_measures_classif.ce
```

We can request the benchmark to show us multiple of these measures:

```{r}
res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.auc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))
```

### Advanced look at models within

We can examine in depth the results by getting out the models fitted in each fold:

```{r}
# eg get the trees (2nd model fitted), by asking for second set of resample
# results
trees <- res$resample_result(2)

# Then, let's look at the tree from first CV iteration, for example:
tree1 <- trees$learners[[1]]

# This is a fitted rpart object, so we can look at the model within
tree1_rpart <- tree1$model

# If you look in the rpart package documentation, it tells us how to plot the
# tree that was fitted
plot(tree1_rpart, compress = TRUE, margin = 0.1)
text(tree1_rpart, use.n = TRUE, cex = 0.8)
```

We can see the other trees too.  Change the 5 in double brackets [[]] below to other values from 1 to 5 to see the model from each round of cross validation.

```{r}
plot(res$resample_result(2)$learners[[5]]$model, compress = TRUE, margin = 0.1)
text(res$resample_result(2)$learners[[5]]$model, use.n = TRUE, cex = 0.8)
```

It may be that these trees need to be pruned.  To do this, we would need to enable the cross-validation option to `rpart` in the learner.  We can fit this individually and make a selection for the cost penalty (see alpha in lectures), before then setting this value when benchmarking (NOTE: this is not quite optimal but MLR3 doesn't yet have the option for us to select this within folds ... coming soon hopefully).

In particular, note we are now doing *nested* cross validation which is the correct way to do parameter selection without biasing test error.  Change the 5 in double brackets [[]] to other values from 1 to 5 to see cross validation plot from each round.

```{r}
# Enable cross validation
lrn_cart_cv <- lrn("classif.rpart", predict_type = "prob", xval = 10)

res_cart_cv <- resample(credit_task, lrn_cart_cv, cv5, store_models = TRUE)
rpart::plotcp(res_cart_cv$learners[[5]]$model)
```

Now, choose a cost penalty and add this as a model to our benchmark set:

```{r}
lrn_cart_cp <- lrn("classif.rpart", predict_type = "prob", cp = 0.016)

res <- benchmark(data.table(
  task       = list(credit_task),
  learner    = list(lrn_baseline,
                    lrn_cart,
                    lrn_cart_cp),
  resampling = list(cv5)
), store_models = TRUE)

res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.auc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))
```

In this case we see a slight improvement to false-positive rate at the cost of higher errors elsewhere.  These might be tradeoffs you need to make in the real world.



### Dealing with missingness and factors

To handle missing data and factors, we will need to introduce a modelling pipeline.  In this pipeline we need to impute missing values and dummy/one-hot code factors.

Pipelines allow us to create a sophisticated workflow without having to manually code how everything ties together.  To see what pipeline operations are available:

```{r}
# View will open a tab where you can more easily see the detail ...
View(as.data.table(mlr_pipeops))
# ... whilst this just gives the names
mlr_pipeops

# Again, to see help on any of them, prefix the key name with mlr_pipeops_
?mlr_pipeops_encode
```

So we can see the encode pipeline can do one-hot encoding of factors.  We'll do this first.  XGBoost doesn't accept factors (look back at learners table earlier), so we now create a pipeline operation to encode them before passing to the learner.  the function `po()` adds operations and `%>>%` connects the steps

```{r}
# Create a pipeline which encodes and then fits an XGBoost model
lrn_xgboost <- lrn("classif.xgboost", predict_type = "prob")
pl_xgb <- po("encode") %>>%
  po(lrn_xgboost)

# Now fit as normal ... we can just add it to our benchmark set
res <- benchmark(data.table(
  task       = list(credit_task),
  learner    = list(lrn_baseline,
                    lrn_cart,
                    lrn_cart_cp,
                    pl_xgb),
  resampling = list(cv5)
), store_models = TRUE)

res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))
```

Handling missingness is slightly more involved.  We provide a pipeline recipie here which is quite robust ... read the documentation of each step to understand more.

We then apply this to logistic regression.

```{r}
# First create a pipeline of just missing fixes we can later use with models
pl_missing <- po("fixfactors") %>>%
  po("removeconstants") %>>%
  po("imputesample", affect_columns = selector_type(c("ordered", "factor"))) %>>%
  po("imputemean")

# Now try with a model that needs no missingness
lrn_log_reg <- lrn("classif.log_reg", predict_type = "prob")
pl_log_reg <- pl_missing %>>%
  po(lrn_log_reg)

# Now fit as normal ... we can just add it to our benchmark set
res <- benchmark(data.table(
  task       = list(credit_task),
  learner    = list(lrn_baseline,
                    lrn_cart,
                    lrn_cart_cp,
                    pl_xgb,
                    pl_log_reg),
  resampling = list(cv5)
), store_models = TRUE)

res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))
```

To improve these results we really need to spend some time tuning the constitutent models, for example gradient boosting should be able to perform better than this and we would want to examine the number of boosting rounds etc being employed.

### Advanced: super learning

Rather than having to choose among the models that we fitted above, we could instead fit all of them and have a final "super learner" fitted which automatically selects the best prediction based on the available base learners.  We can do this using the pipelines in MLR3 ...

We start from scratch to make this more advanced example self contained.

```{r}
library("data.table")
library("mlr3verse")

set.seed(212) # set seed for reproducibility

# Load data
data("credit_data", package = "modeldata")

# Define task
credit_task <- TaskClassif$new(id = "BankCredit",
                               backend = credit_data,
                               target = "Status",
                               positive = "bad")

# Cross validation resampling strategy
cv5 <- rsmp("cv", folds = 5)
cv5$instantiate(credit_task)

# Define a collection of base learners
lrn_baseline <- lrn("classif.featureless", predict_type = "prob")
lrn_cart     <- lrn("classif.rpart", predict_type = "prob")
lrn_cart_cp  <- lrn("classif.rpart", predict_type = "prob", cp = 0.016, id = "cartcp")
lrn_ranger   <- lrn("classif.ranger", predict_type = "prob")
lrn_xgboost  <- lrn("classif.xgboost", predict_type = "prob")
lrn_log_reg  <- lrn("classif.log_reg", predict_type = "prob")

# Define a super learner
lrnsp_log_reg <- lrn("classif.log_reg", predict_type = "prob", id = "super")

# Missingness imputation pipeline
pl_missing <- po("fixfactors") %>>%
  po("removeconstants") %>>%
  po("imputesample", affect_columns = selector_type(c("ordered", "factor"))) %>>%
  po("imputemean")

# Factors coding pipeline
pl_factor <- po("encode")

# Now define the full pipeline
spr_lrn <- gunion(list(
  # First group of learners requiring no modification to input
  gunion(list(
    po("learner_cv", lrn_baseline),
    po("learner_cv", lrn_cart),
    po("learner_cv", lrn_cart_cp)
  )),
  # Next group of learners requiring special treatment of missingness
  pl_missing %>>%
    gunion(list(
      po("learner_cv", lrn_ranger),
      po("learner_cv", lrn_log_reg),
      po("nop") # This passes through the original features adjusted for
                # missingness to the super learner
    )),
  # Last group needing factor encoding
  pl_factor %>>%
    po("learner_cv", lrn_xgboost)
)) %>>%
  po("featureunion") %>>%
  po(lrnsp_log_reg)

# This plot shows a graph of the learning pipeline
spr_lrn$plot()

# Finally fit the base learners and super learner and evaluate
res_spr <- resample(credit_task, spr_lrn, cv5, store_models = TRUE)
res_spr$aggregate(list(msr("classif.ce"),
                       msr("classif.acc"),
                       msr("classif.fpr"),
                       msr("classif.fnr")))
```

You will note these are the best results achieved of all the learners (except in false positive), albeit that this is by far the most complicated model
